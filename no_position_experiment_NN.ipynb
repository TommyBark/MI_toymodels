{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# pio.renderers.default = \"colab\"\n",
    "import tqdm.auto as tqdm\n",
    "import einops\n",
    "from transformer_lens.utils import to_numpy\n",
    "from transformer_lens import EasyTransformer, EasyTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(tensor, line_labels=None, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = to_numpy(tensor)\n",
    "    labels = {\"y\": yaxis, \"x\": xaxis}\n",
    "    fig = px.line(tensor, labels=labels, **kwargs)\n",
    "    if line_labels:\n",
    "        for c, label in enumerate(line_labels):\n",
    "            fig.data[c].name = label\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def imshow(tensor, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = to_numpy(tensor)\n",
    "    plot_kwargs = {\n",
    "        \"color_continuous_scale\": \"RdBu\",\n",
    "        \"color_continuous_midpoint\": 0.0,\n",
    "        \"labels\": {\"x\": xaxis, \"y\": yaxis},\n",
    "    }\n",
    "    plot_kwargs.update(kwargs)\n",
    "    px.imshow(tensor, **plot_kwargs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = EasyTransformerConfig(\n",
    "    n_layers=2,\n",
    "    d_model=64,\n",
    "    d_head=64,\n",
    "    n_heads=1,\n",
    "    d_mlp=256,\n",
    "    d_vocab=300,\n",
    "    n_ctx=50,\n",
    "    act_fn=\"relu\",\n",
    "    normalization_type=\"LN\",\n",
    ")\n",
    "model = EasyTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deactivate_position(model):\n",
    "    model.pos_embed.W_pos.data[:] = 0.0\n",
    "    model.pos_embed.W_pos.requires_grad = False\n",
    "\n",
    "\n",
    "deactivate_position(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,  93,  34, 155, 274, 116, 114, 248,  68,   3, 298,  83, 194,  20,\n",
      "           8, 133,  32,  66,  62,  73, 210, 273,  46, 243, 104, 232, 161, 125,\n",
      "         123, 251,   7,   4, 115, 127,  21,   1,  89, 142,   6,  15, 298, 251,\n",
      "          88, 229, 108, 114,  23,  88,   3, 265],\n",
      "        [  0, 118,  46, 274, 105, 268, 131,  35,  19,  58, 226, 278,  27,  25,\n",
      "         276, 180, 164,   4,  95,  27,  74, 201, 105,  65,  80, 185,  44, 258,\n",
      "         105,  60,  58,  47, 126,  60, 294, 253, 258, 136,  29, 101, 258,  77,\n",
      "          80, 180, 159, 169, 122, 117,  27, 194]])\n"
     ]
    }
   ],
   "source": [
    "def make_data_generator(cfg, batch_size, seed=123, incl_bos_token=True):\n",
    "    torch.manual_seed(seed)\n",
    "    while True:\n",
    "        x = torch.randint(1, cfg.d_vocab, (batch_size, cfg.n_ctx))\n",
    "        if incl_bos_token:\n",
    "            x[:, 0] = 0\n",
    "        yield x\n",
    "\n",
    "\n",
    "data_generator = make_data_generator(cfg, 2)\n",
    "print(next(data_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, tokens, return_per_token=False):\n",
    "    # logit shape: [batch, pos, vocab]\n",
    "    # token shape: [batch, pos]\n",
    "    logits = logits[:, 1:]\n",
    "    tokens = tokens[:, :-1]\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=tokens[..., None])[\n",
    "        ..., 0\n",
    "    ]  # collects the log_probs for tokens of interest -> true tokens\n",
    "    if return_per_token:\n",
    "        return -correct_log_probs\n",
    "    else:\n",
    "        return -correct_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0004, 0.0003, 0.0031, 0.0005]])\n",
      "tensor(0.0011)\n"
     ]
    }
   ],
   "source": [
    "# Test the loss function works\n",
    "test_tokens = torch.arange(5)[None, :]\n",
    "test_logits = torch.randn(1, 5, 10)\n",
    "test_logits[:, 1, 0] = 10.0\n",
    "test_logits[:, 2, 1] = 10.0\n",
    "test_logits[:, 3, 2] = 10.0\n",
    "test_logits[:, 4, 3] = 10.0\n",
    "print(loss_fn(test_logits, test_tokens, return_per_token=True))\n",
    "print(loss_fn(test_logits, test_tokens, return_per_token=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 4000\n",
    "lr = 1e-4\n",
    "betas = (0.9, 0.95)\n",
    "max_grad_norm = 1.0\n",
    "wd = 0.1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=wd)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: min(i / 100, 1.0))\n",
    "\n",
    "data_loader = make_data_generator(cfg, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_PATH = \"trained_model.pkl\"\n",
    "if os.path.exists(trained_model_PATH):\n",
    "    model.load_state_dict(torch.load(trained_model_PATH))\n",
    "else:\n",
    "    losses = []\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        tokens = next(data_loader)\n",
    "        # tokens = tokens.cuda()\n",
    "        logits = model(tokens)\n",
    "        loss = loss_fn(logits, tokens)\n",
    "        loss.backward()\n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: {loss.item()}\")\n",
    "    px.line(losses, labels={\"x\": \"Epoch\", \"y\": \"Loss\"})\n",
    "    if not os.path.exists(trained_model_PATH):\n",
    "        torch.save(model.state_dict(), trained_model_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6475)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pos_embed.W_pos.norm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Give model some data\n",
    "big_data_loader = make_data_generator(cfg, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how different parts of model contribute to the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the hypothesis is correct, try to interpret MLPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
